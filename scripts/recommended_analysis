#!/bin/bash

echo "Preparing data for training..."
# Split up the data for processing
for i in $(seq 1 13); do
	base="ctu13/$i"
	ben_file="$base/benign.txt"
	bot_file="$base/bot.txt"

	# Split the benign file into smaller files for easier processing
	split -l 10000 $ben_file
	if [[ ! -d $base/benign_batched ]]; then
		mkdir $base/benign_batched
	fi
	mv x* $base/benign_batched
done

if [[ ! -d data ]]; then
	mkdir data
fi

if [[ ! -f data/tokenizer-13.pickle ]]; then
	echo "Building tokenizer..."
	python scripts/build_tokenizer.py
fi

echo "Starting to train model..."
last_file=""
for i in $(seq 1 13); do
	log_file="data/${i}_log.out"
	if [[ ! -f $log_file ]]; then
		touch $log_file
	fi
	for in_file in $(find ctu13/$i/benign_batched -type f); do
		if grep -q "$in_file" $log_file; then
			continue
		fi
		echo "Processing $in_file..." >>$log_file
		python src/main.py --benign-path $in_file --bot-path ctu13/$i/bot.txt --epochs 25 --tokenizer-path data/tokenizer-13.pickle &>>$log_file
		printf "\n\n\n" >>$log_file
		last_file=$in_file
	done
done

echo "Running model evaluations..."
for i in $(seq 1 13); do
	echo "Evaluationg ctu13/$i:"
	python src/main.py --benign-path $last_file --bot-path ctu13/$i/bot.txt --skip-training --evaluate
done
