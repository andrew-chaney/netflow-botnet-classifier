#!/bin/bash

split_size=50000

echo "Preparing data for training..."
# Split up the data
for i in $(seq 1 13); do
	base="ctu13/$i"
	in_file="$base/merged.txt"
	ben_file="$base/benign.txt"
	bot_file="$base/bot.txt"

	if [[ -d $base/benign_batched ]]; then
		continue
	fi

	mkdir $base/benign_batched

	# Key difference with micro is training on a smaller sample
	if [[ $(wc -l $ben_file | awk '{print $1}') -gt $split_size ]]; then
		shuf -n $split_size $ben_file -o $base/new_ben.txt
		mv $base/new_ben.txt $ben_file
		rm $base/new_ben.txt
	fi

	# Split the benign file into smaller files for easier processing
	split -l 10000 $ben_file
	mv x* $base/benign_batched
done

if [[ ! -d data ]]; then
	mkdir data
fi

if [[ ! -f data/tokenizer-13.pickle ]]; then
	echo "Building tokenizer..."
	python scripts/build_tokenizer.py
fi

echo "Starting to train model..."
last_file=""
for i in $(seq 1 13); do
	log_file="data/training_logs/${i}_log.out"

	if [[ ! -d data/training_logs ]]; then
		mkdir data/training_logs
	fi

	if [[ ! -f $log_file ]]; then
		touch $log_file
	fi

	for in_file in $(find ctu13/$i/benign_batched -type f); do
		if grep -q "$in_file" $log_file; then
			continue
		fi
		echo "Processing $in_file..." >>$log_file
		python src/main.py --benign-path $in_file --bot-path ctu13/$i/bot.txt --epochs 25 --tokenizer-path data/tokenizer-13.pickle &>>$log_file
		printf "\n\n\n" >>$log_file
		last_file=$in_file
	done
done

echo "Running model evaluations..."
for i in $(seq 1 13); do
	log_file="data/evaluation_logs/${i}_log.out"

	if [[ ! -d data/evaluation_logs ]]; then
		mkdir data/evaluation_logs
	fi

	if [[ ! -f $log_file ]]; then
		touch $log_file
	fi

	echo "Evaluationg ctu13/$i:"
	python src/main.py --benign-path $last_file --bot-path ctu13/$i/bot.txt --skip-training --evaluate &>>$log_file
done
