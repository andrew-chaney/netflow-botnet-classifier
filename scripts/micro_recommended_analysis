#!/bin/bash

split_size=250000

# Split up the data
for i in $(seq 1 13); do
	base="ctu13/$i"
	in_file="$base/merged.txt"
	ben_file="$base/benign.txt"
	bot_file="$base/bot.txt"

	# Separate out the benign entries
	awk -F ',' '{ if ($4==0) { print $0 }}' $in_file >$ben_file

	# Key difference with micro is training on a smaller sample
	if [[ $(wc -l $ben_file | awk '{print $1}') -gt $split_size ]]; then
		shuf -n $split_size $ben_file -o $base/new_ben.txt
		mv $base/new_ben.txt $ben_file
	fi

	# Separate out the bot entries
	awk -F ',' '{ if ($4==1) { print $0 }}' $in_file >$bot_file

	# Split the benign file into smaller files for easier processing
	split -l 10000 $ben_file
	if [[ ! -d $base/benign_batched ]]; then
		mkdir $base/benign_batched
	fi
	mv x* $base/benign_batched
done

if [[ ! -d data ]]; then
	mkdir data
fi

if [[ ! -f data/tokenizer.pickle ]]; then
	echo "Building tokenizer..."
	python scripts/build_tokenizer.py
fi

echo "Starting to train model..."
last_file=""
for i in $(seq 1 13); do
	log_file="data/${i}_log.out"
	if [[ ! -f $log_file ]]; then
		touch $log_file
	fi
	for in_file in $(find ctu13/$i/benign_batched -type f); do
		if grep -q "$in_file" $log_file; then
			continue
		fi
		echo "Processing $in_file..." >>$log_file
		python src/main.py --benign-path $in_file --bot-path ctu13/$i/bot.txt --epochs 25 &>>$log_file
		printf "\n\n\n" >>$log_file
		last_file=$in_file
	done
done

echo "Running model evaluations..."
for i in $(seq 1 13); do
	python src/main.py --benign-path $last_file --bot-path ctu13/$i/bot.txt --skip-training --evaluate
done
